{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_file=pd.read_csv('path/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "new_file['pickup_datetime']=pd.to_datetime(new_file['pickup_datetime'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_file['dropoff_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plb\n",
    "\n",
    "plt.hist(new_file['trip_duration'], 100, (1,5000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file['log_trip_duration'] = [(np.log1p(i)) for i in new_file['trip_duration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_file['log_trip_duration'], 70, (1,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltd=new_file['log_trip_duration']\n",
    "sum_sq=(((ltd-ltd.mean())**2).sum()/len(new_file))**(1/2)\n",
    "sum_sq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "new_file['month'] = new_file['pickup_datetime'].dt.month\n",
    "new_file['weekday'] = new_file['pickup_datetime'].dt.weekday\n",
    "new_file['hour'] = new_file['pickup_datetime'].dt.hour\n",
    "new_file['day'] = new_file['pickup_datetime'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = np.empty((7, 31))\n",
    "for i in range(7): \n",
    "    for l in range(31): \n",
    "        msk = (new_file.loc[:, 'day'] == l) & (new_file.loc[:, 'month'] == i)\n",
    "        s = msk.sum()\n",
    "        quant[i, l] = s\n",
    "plt.figure(figsize=(25, 12))\n",
    "sns.heatmap(quant).invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "new_file['counter'] = np.ones(len(new_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndays_2 =len(np.unique(new_file['pickup_datetime'].dt.date)) \n",
    "plt.plot(new_file.groupby(['weekday']).counter.sum()/ ndays_2) \n",
    "plt.show()\n",
    "\n",
    "ndays_3 =len(np.unique(new_file['pickup_datetime'].dt.date)) \n",
    "plt.plot(new_file.groupby(['hour']).counter.sum()/ ndays_3) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndays_3 =len(np.unique(new_file['pickup_datetime'].dt.date)) \n",
    "h_m= np.empty((24, 7))\n",
    "for i in range(24): \n",
    "    for l in range(7): \n",
    "        msk = (new_file.loc[:, 'hour'] == i) & (new_file.loc[:, 'month'] == l)\n",
    "        s = msk.sum()\n",
    "        h_m[i, l] = s     \n",
    "sns.heatmap(h_m).invert_yaxis()\n",
    "plt.show()          \n",
    "    \n",
    "h_wd = np.empty((24, 7))\n",
    "for i in range(24): \n",
    "    for l in range(7): \n",
    "        msk = (new_file.loc[:, 'hour'] == i) & (new_file.loc[:, 'weekday'] == l)\n",
    "        s = msk.sum()\n",
    "        h_wd[i, l] = s \n",
    "sns.heatmap(h_wd).invert_yaxis()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous=[]\n",
    "date=new_file['pickup_datetime'].dt.date\n",
    "date_lst=[str(i) for i in date ]\n",
    "for i in date_lst:\n",
    "    if i == '2016-01-24' or i == '2016-01-23' or i =='2016-05-30':\n",
    "        anomalous.append(True)\n",
    "    else:\n",
    "        anomalous.append(False)\n",
    "new_file['anomalous']=anomalous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_file['counter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(new_file)) < 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge,LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "categ_features=['anomalous']\n",
    "numeric_features = ['month','weekday','hour','day','passenger_count']\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categ_features),\n",
    "    ('scaling', StandardScaler(), numeric_features)\n",
    "])\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('ohe_and_scaling', column_transformer),\n",
    "    ('regression', Ridge())\n",
    "])\n",
    "\n",
    "model = pipeline.fit(new_file.loc[mask], new_file.loc[mask,'log_trip_duration'])\n",
    "y_pred = model.predict(new_file.loc[~mask])\n",
    "\n",
    "print(\"Test RMSLE = %.4f\" % ((mean_squared_error(new_file.loc[~mask,'log_trip_duration'], \\\n",
    "                                                 y_pred)))**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, Circle, LayerGroup, basemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circles_map(data, latitude_column, longitude_column, color):\n",
    "    \n",
    "    center = (new_file[latitude_column].mean(),new_file[longitude_column].mean())\n",
    "    res_map = Map(center=center, zoom=10, basemap=basemaps.Esri.NatGeoWorldMap)\n",
    "    \n",
    "    circles = []\n",
    "    for _, row in data.iterrows():\n",
    "        circles.append(Circle(\n",
    "            location=(row[latitude_column], row[longitude_column]),\n",
    "            fill_color=color,\n",
    "            fill_opacity=0.2,\n",
    "            radius=300,\n",
    "            stroke=False\n",
    "        ))\n",
    "    c_layer = LayerGroup(layers=circles)\n",
    "    res_map.add_layer(c_layer)\n",
    "\n",
    "    return res_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles_map(new_file.sample(1000), \"pickup_latitude\", \"pickup_longitude\", \"purple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles_map(new_file.sample(1000), \"dropoff_latitude\", \"dropoff_longitude\", \"purple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "pickuplt=new_file.pickup_latitude.tolist()\n",
    "pickuplg=new_file.pickup_longitude.tolist()\n",
    "dropofft=new_file.dropoff_latitude.tolist()\n",
    "dropoffg=new_file.dropoff_longitude.tolist()\n",
    "\n",
    "for_haversine_p=list(zip(pickuplt,pickuplg))\n",
    "for_haversine_d=list(zip(dropofft,dropoffg))\n",
    "haversine=list(map(lambda x,y: haversine(x,y),for_haversine_p,for_haversine_d))\n",
    "new_file['haversine']=haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_haversine=[np.log1p(i) for i in haversine]\n",
    "new_file['log_haversine']=log_haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "with_log = scipy.stats.pearsonr(new_file['log_haversine'],new_file['log_trip_duration'])[0]\n",
    "witht_log = scipy.stats.pearsonr(new_file['haversine'],new_file['log_trip_duration'])[0]\n",
    "print(with_log)\n",
    "print(witht_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_velocity=new_file['haversine']/(new_file['trip_duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_velocity,50, (0.001,12.960528e-03))\n",
    "mean_velocity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file['mean_velocity']=mean_velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_speed = np.empty((7, 24)) \n",
    "for wd in range(7):\n",
    "    for h in range(24): \n",
    "        mk = (new_file.loc[:, 'weekday'] == wd) & (new_file.loc[:, 'hour'] == h)\n",
    "        s = new_file.loc[mk, 'haversine'] / new_file.loc[mk, 'trip_duration']\n",
    "        m_speed[wd, h] = np.median(s)\n",
    "sns.heatmap(m_speed).invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_file['mean_velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file['with_traffic_jams'] = m_speed[new_file.weekday, new_file.hour] < 0.0035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_Kennedy=[40.64,40.65,-73.79,-73.78]\n",
    "lim_Guardia=[40.76,40.78,-73.88,-73.87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file['from_Kennedy'] = (new_file.pickup_latitude  > lim_Kennedy[0]) & \\\n",
    "                           (new_file.pickup_latitude  < lim_Kennedy[1]) & \\\n",
    "                           (new_file.pickup_longitude > lim_Kennedy[2]) & \\\n",
    "                           (new_file.pickup_longitude < lim_Kennedy[3])\n",
    "\n",
    "new_file['to_Kennedy'] = (new_file.dropoff_latitude  > lim_Kennedy[0]) & \\\n",
    "                         (new_file.dropoff_latitude  < lim_Kennedy[1]) & \\\n",
    "                         (new_file.dropoff_longitude > lim_Kennedy[2]) & \\\n",
    "                         (new_file.dropoff_longitude < lim_Kennedy[3])\n",
    "\n",
    "new_file['from_Guardia'] = (new_file.pickup_latitude  > lim_Guardia[0]) & \\\n",
    "                           (new_file.pickup_latitude  < lim_Guardia[1]) & \\\n",
    "                           (new_file.pickup_longitude > lim_Guardia[2]) & \\\n",
    "                           (new_file.pickup_longitude < lim_Guardia[3])\n",
    "\n",
    "new_file['to_Guardia'] = (new_file.dropoff_latitude  > lim_Guardia[0]) & \\\n",
    "                         (new_file.dropoff_latitude  < lim_Guardia[1]) & \\\n",
    "                         (new_file.dropoff_longitude > lim_Guardia[2]) & \\\n",
    "                         (new_file.dropoff_longitude < lim_Guardia[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_file['for_haversine_p']=for_haversine_p\n",
    "new_file['for_haversine_d']=for_haversine_d\n",
    "\n",
    "lim_Kennedy=[40.64,40.65,-73.79,-73.78]\n",
    "lim_Guardia=[40.76,40.78,-73.88,-73.87]\n",
    "\n",
    "\n",
    "pk=[]\n",
    "for i,l in new_file['for_haversine_p']:\n",
    "    if lim_Kennedy[0]<=i <=lim_Kennedy[1]  and  lim_Kennedy[2]>=l>= lim_Kennedy[3]:\n",
    "        pk.append(True) \n",
    "    else:\n",
    "        pk.append(False) \n",
    "\n",
    "dk=[]\n",
    "for i,l in new_file['for_haversine_d']:\n",
    "    if lim_Kennedy[0]<=i <=lim_Kennedy[1]  and  lim_Kennedy[2]>=l>= lim_Kennedy[3]:\n",
    "        dk.append(True) \n",
    "    else:\n",
    "        dk.append(False) \n",
    "\n",
    "\n",
    "pg=[]\n",
    "for i,l in new_file['for_haversine_p']:\n",
    "    if lim_Guardia[0]<=i <=lim_Guardia[1]  and  lim_Guardia[2]>=l>= lim_Guardia[3]:\n",
    "        pg.append(True) \n",
    "    else:\n",
    "        pg.append(False) \n",
    "\n",
    "dg=[]\n",
    "for i,l in new_file['for_haversine_d']:\n",
    "    if lim_Guardia[0]<=i <=lim_Guardia[1]  and  lim_Guardia[2]>=l>= lim_Guardia[3]:\n",
    "        dg.append(True) \n",
    "    else:\n",
    "        dg.append(False) \n",
    "        \n",
    "        \n",
    "new_file['from_Kennedy']=pk\n",
    "new_file['to_Kennedy']=dk\n",
    "new_file['from_Guardia']=pg\n",
    "new_file['to_Guardia']=dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_file['for_haversine_p']\n",
    "del new_file['for_haversine_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=new_file.from_Guardia, y=new_file.log_trip_duration) \n",
    "plt.show()\n",
    "sns.boxplot(x=new_file.to_Guardia, y=new_file.log_trip_duration) \n",
    "plt.show()\n",
    "sns.boxplot(x=new_file.from_Kennedy, y=new_file.log_trip_duration) \n",
    "plt.show()\n",
    "sns.boxplot(x=new_file.to_Kennedy, y=new_file.log_trip_duration) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapGridTransformer:\n",
    "    def __init__(self, nrows=4, ncols=3):\n",
    "        self.nrows = nrows\n",
    "        self.ncols = ncols\n",
    "    def fit(self, X, y=None):\n",
    "        X_ = X.values\n",
    "        self.b, self.l = np.median(X_, axis=0) - X_.std(axis=0)*1\n",
    "        self.t, self.r = np.median(X_, axis=0) + X_.std(axis=0)*1\n",
    "        self.row_h = (self.t - self.b) / self.nrows\n",
    "        self.col_w = (self.r - self.l) / self.ncols\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_ = X.values\n",
    "        c = (X_[:, 1] - self.l) // self.col_w\n",
    "        r = (X_[:, 0] - self.b) // self.row_h\n",
    "        n = c + r * self.ncols + 1\n",
    "        n[(X_[:, 1] < self.l) + (X_[:, 1] > self.r) + \\\n",
    "          (X_[:, 0] < self.b) + (X_[:, 0] > self.t)] = 0\n",
    "        return n.astype(int)\n",
    "    def fit_transform(self, X ,y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "t = MapGridTransformer()\n",
    "grid_pickup = t.fit_transform(new_file.loc[:, ['pickup_longitude', 'pickup_latitude']])\n",
    "grid_dropoff = t.fit_transform(new_file.loc[:, ['dropoff_longitude', 'dropoff_latitude']])\n",
    "new_file['grid_pickup'] = grid_pickup\n",
    "new_file['grid_dropoff'] = grid_dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_file.log_haversine != np.log1p(new_file.haversine)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file.log_haversine = np.log1p(new_file.haversine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categ_features=['anomalous','with_traffic_jams',\n",
    "                'from_Kennedy','to_Kennedy','from_Guardia','to_Guardia']\n",
    "numeric_features = ['grid_pickup','grid_dropoff','log_haversine','month','weekday','hour','day',\n",
    "                    'passenger_count']\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categ_features),\n",
    "    ('scaling', StandardScaler(), numeric_features)\n",
    "])\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('ohe_and_scaling', column_transformer),\n",
    "    ('regression', Ridge())\n",
    "])\n",
    "\n",
    "#-----------------------------------------------------------------------------------------#\n",
    "\n",
    "model = pipeline.fit(new_file.loc[mask], new_file.loc[mask,'log_trip_duration'])\n",
    "y_pred = model.predict(new_file.loc[~mask])\n",
    "print(\"Test RMSLE = %.4f\" % ((mean_squared_error(new_file.loc[~mask,'log_trip_duration'], \\\n",
    "                                                 y_pred))**(1/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_file['vendor_id'].unique()))\n",
    "print(len(new_file['passenger_count'].unique()))\n",
    "print(len(new_file['store_and_fwd_flag'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=new_file['vendor_id'], y=new_file.log_trip_duration) \n",
    "plt.show()\n",
    "sns.boxplot(x=new_file['passenger_count'], y=new_file.log_trip_duration) \n",
    "plt.show()\n",
    "sns.boxplot(x=new_file['store_and_fwd_flag'], y=new_file.log_trip_duration) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file.vendor_id -=1\n",
    "new_file.store_and_fwd_flag = (new_file.store_and_fwd_flag == 'Y').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_features=['anomalous','with_traffic_jams','from_Kennedy','to_Kennedy','from_Guardia','to_Guardia']\n",
    "numeric_features = ['vendor_id','store_and_fwd_flag','month',\n",
    "                        'weekday','hour','day','passenger_count','log_haversine']\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categ_features),\n",
    "    ('scaling', StandardScaler(), numeric_features)\n",
    "])\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('ohe_and_scaling', column_transformer),\n",
    "    ('regression', Ridge())\n",
    "])\n",
    "\n",
    "\n",
    "model = pipeline.fit(new_file.loc[mask], new_file.loc[mask,'log_trip_duration'])\n",
    "y_pred = model.predict(new_file.loc[~mask])\n",
    "\n",
    "print(\"Test RMSLE = %.4f\" % ((mean_squared_error(new_file.loc[~mask,'log_trip_duration'], \\\n",
    "                                                 y_pred)))**(1/2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_file.trip_duration,100)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file=new_file[new_file.trip_duration < 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(new_file)) < 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_file.trip_duration,100)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_file.haversine,100)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_file.passenger_count,10)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt=new_file.from_Kennedy+new_file.to_Kennedy+new_file.from_Guardia+new_file.to_Guardia\n",
    "plt.hist(new_file.trip_duration.loc[~mt],100,(0,100000))\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_file.passenger_count>6).sum()\n",
    "new_file.passenger_count[new_file.passenger_count>6]=0\n",
    "\n",
    "new_file['airport_trip'] = new_file['from_Guardia'] | \\\n",
    "                new_file['from_Kennedy'] | new_file['to_Guardia'] | new_file['to_Kennedy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_file['from_Guardia'] \n",
    "del new_file['from_Kennedy']  \n",
    "del new_file['to_Guardia']  \n",
    "del new_file['to_Kennedy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file['traffic_jams']=new_file['with_traffic_jams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_file['with_traffic_jams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del  new_file['haversine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_features=['anomalous','traffic_jams','airport_trip']\n",
    "numeric_features = ['vendor_id','passenger_count','store_and_fwd_flag','month',\n",
    "                        'weekday','hour','day','log_haversine']\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categ_features),\n",
    "    ('scaling', StandardScaler(), numeric_features)\n",
    "])\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('ohe_and_scaling', column_transformer),\n",
    "    ('regression', Ridge())\n",
    "])\n",
    "\n",
    "\n",
    "model = pipeline.fit(new_file.loc[mask], new_file.loc[mask,'log_trip_duration'])\n",
    "y_pred = model.predict(new_file.loc[~mask])\n",
    "\n",
    "print(\"Test RMSLE = %.4f\" % ((mean_squared_error(new_file.loc[~mask,'log_trip_duration'], \\\n",
    "                                                 y_pred)))**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(categ_features)+len(numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "numeric_features = categ_features=['anomalous','traffic_jams','airport_trip']\n",
    "numeric_features = ['vendor_id','passenger_count','store_and_fwd_flag','month',\n",
    "                        'weekday','hour','day','log_haversine']\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categ_features),\n",
    "    ('scaling', StandardScaler(), numeric_features)\n",
    "])\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('ohe_and_scaling', column_transformer),\n",
    "    ('regression', Lasso())\n",
    "])\n",
    "\n",
    "model = pipeline.fit(new_file.loc[mask], new_file.loc[mask,'log_trip_duration'])\n",
    "y_pred = model.predict(new_file.loc[~mask])\n",
    "\n",
    "print(\"Test RMSLE = %.4f\" % ((mean_squared_error(new_file.loc[~mask,'log_trip_duration'], \\\n",
    "                                                 y_pred)))**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    error = (y_true - y_pred) ** 2\n",
    "    return np.sqrt(np.mean(error))\n",
    "\n",
    "rmse_scorer = make_scorer(\n",
    "    rmse,\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "alphas = np.logspace(-4, -1, 10)\n",
    "searcher = GridSearchCV(pipeline, [{\"regression__alpha\": alphas}], \n",
    "                        scoring=rmse_scorer, cv=5)\n",
    "\n",
    "m = np.random.rand(len(new_file)) < 0.2\n",
    "searcher.fit(new_file.loc[m], \n",
    "             new_file.loc[m, 'log_trip_duration'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = searcher.best_params_['regression__alpha']\n",
    "print(\"Best alpha = %.4f\" % best_alpha)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.plot(alphas, -searcher.cv_results_['mean_test_score'])\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"CV score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_pairs, counts = np.unique(np.array((new_file.grid_pickup,new_file.grid_dropoff)).T,\n",
    "                               axis=0,return_counts=True)\n",
    "\n",
    "grid_pairs=grid_pairs[counts.argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_p=np.array((new_file.grid_pickup,new_file.grid_dropoff)).T\n",
    "gp=np.empty(len(new_file))\n",
    "for i,l in enumerate(all_p):\n",
    "    gp[i]= np.where(grid_pairs==l)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_grid=[]\n",
    "for i in gp:\n",
    "    if i!=0:\n",
    "        gp_grid.append(-1)\n",
    "    else:\n",
    "        gp_grid.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file['gp_grid']=gp_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "categ_features=['anomalous','traffic_jams','airport_trip']\n",
    "numeric_features = ['passenger_count','month','gp_grid','weekday','hour','log_haversine']\n",
    "\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categ_features),\n",
    "    ('scaling', StandardScaler(), numeric_features)\n",
    "])\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('ohe_and_scaling', column_transformer),\n",
    "    ('regression', Lasso(best_alpha))\n",
    "])\n",
    "\n",
    "\n",
    "model = pipeline.fit(new_file.loc[mask], new_file.loc[mask,'log_trip_duration'])\n",
    "y_pred = model.predict(new_file.loc[~mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, -1, 10)\n",
    "searcher = GridSearchCV(pipeline, [{\"regression__alpha\": alphas}], \n",
    "                        scoring=rmse_scorer, cv=5)\n",
    "\n",
    "n = np.random.rand(len(new_file)) < 0.2\n",
    "searcher.fit(new_file.loc[n], \n",
    "             new_file.loc[n, 'log_trip_duration'])\n",
    "\n",
    "b_alpha = searcher.best_params_['regression__alpha']\n",
    "print(\"Best alpha = %.4f\" % b_alpha)\n",
    "print('RMSLE score:{}'.format(-searcher.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test RMSLE = %.4f\" % ((mean_squared_error(new_file.loc[~mask,'log_trip_duration'], \\\n",
    "                                                 y_pred)))**(1/2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
